{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![faces](FER+vsFER.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogLeNet [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 딥러닝은 망이 깊고(depth) 레이어가 넓을 수록(wide) 성능이 좋다.\n",
    "* 현실에서는 과최적화나 그레디언트 소멸문제가 발생\n",
    "* 망내 연결을 줄이면서 행렬연산에서는 dense하게 연산되는 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GLN](f02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중간에 추가된 1$\\times$1 convolution layer에 의해 계산상의 병목현상(bottleneck)을 줄이고 연산 페널티를 크게 늘리지 않으면서 네트워크의 깊이와 넓이를 확장해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inception](inception_1x1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"450\"\n",
       "            src=\"http://iamaaditya.github.io/2016/03/one-by-one-convolution/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x4878748>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://iamaaditya.github.io/2016/03/one-by-one-convolution/', 1000, 450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inception (dimension reductions) -- b\n",
    "\n",
    "tower_0 = Conv2D(160, (1, 1), padding='same', activation='relu', kernel_regularizer=l2(reg[1]))(x)\n",
    "\n",
    "tower_1 = Conv2D(112, (1, 1), padding='same', activation='relu', kernel_regularizer=l2(reg[1]))(x)\n",
    "tower_1 = Conv2D(224, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(reg[0]))(tower_1)\n",
    "\n",
    "tower_2 = Conv2D(24, (1, 1), padding='same', activation='relu', kernel_regularizer=l2(reg[1]))(x)\n",
    "tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(reg[0]))(tower_2)\n",
    "\n",
    "tower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu', kernel_regularizer=l2(reg[1]))(tower_3)\n",
    "\n",
    "x = keras.layers.concatenate([tower_0, tower_1, tower_2, tower_3], axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet에서 input data는 224$\\times$224 이미지이다. fer2013의 경우 48 $\\times$ 48 이미지가 input data이기 때문에 적절하게 layer를 줄여야한다.[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](glns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Method | FD | LM | Registration | Illumionation | Accuracy(Public) | Architecture | Depth | Parameters | AD | AF | + Train | + Test | Ensemble |\n",
    "|-------|----|----|-------------|----------------|---------------------------|----|----|-------------|:----:|:----:|:----:|--------|---------|\n",
    "|Method 1| no| no|no | normalize | 71.2%|   CPCPFF| 4 | 12.0 m |  no | no | S, M | -| average |\n",
    "|Method 2| several | no | no | histeq, LPF |72%|   PCCPCCPCFFF | 8 | 6.2 m | no | no | A, M | A| weighted |\n",
    "|Method 3 | no |ref* | rigid (LM) | several | 73.5% |   CPCPCPFF | 5 | 2.4 m |  no | yes | T, M, REG | ten-crop, REG| average |\n",
    "|Method 4| several | ref* |rigid (LM) | several |72.5%|   CPCPCPFF | 5 | 4.8 m |  no | no | T, M | - | hierarchy |\n",
    "|Method 5| no | ref* | affine (LM) | no | 66.5% |  CPCPIIPIPFFF | 11 | 7.3 m | yes | no | ten-crop |-| -|\n",
    "|Method 6| no | ref* | indirect | no | 75% |  CPNCPNCPCFF | 6 | 21.3 m | yes | yes | - |- |- |\n",
    "\n",
    "#### 약자 설명\n",
    "\n",
    "|Preprocessing|Structure|Differences1| Differences2|\n",
    "|:---|:---|:---|:---|\n",
    "|FD: Facial Detection|C: Convolutional|AD: Additional Training Data|S: Similarity Transformation|\n",
    " |LM: Facial Landmark Extraction|P: Pooling| AF: Additional Features |A: Affine Transformation|\n",
    " |HISTEQ: Histogram Equalization|N: Response-Normalization| +: Data augmentation |T:Translation|\n",
    " |LPF: Linear Plane Fitting|I: Inception|  REG: Face Registration |M: Horizontal Mirroring|\n",
    " ||F: Fully connected layers| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![emotion](Plutchik-Model-600.png)\n",
    "\n",
    "* 사람의 인식율: 65$\\pm$5 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments and Future works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 중 얼굴이 아닌 데이터 존재\n",
    "    * 얼굴 인지(Face registration)를 연구하여 성능향상\n",
    "* Image rotation, shearing, resizing 기술 적용\n",
    "* 다양한 데이터셋 시도\n",
    "* 얼굴 특징 인지(Facial Landmark detection)를 연구하여 성능향상(진행중)\n",
    "    * IntraFace 지원종료로 다른 방법 시도(dataset에서 직접 training, 다른 패키지 시도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"450\"\n",
       "            src=\"http://www.humansensing.cs.cmu.edu/intraface/download.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2e8cf60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://www.humansensing.cs.cmu.edu/intraface/download.html', 800, 450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, \"Going Deeper with Convolutions\", https://arxiv.org/abs/1409.4842\n",
    "\n",
    "[2] Ali Mollahosseini, David Chan, Mohammad H. Mahoor, \"Going Deeper in Facial Expression Recognition using Deep Neural Networks\", https://arxiv.org/abs/1511.04110\n",
    "\n",
    "Method 1: [3] Y. Tang, \"Deep Learning using Linear Support Vector Machines\", 2013\n",
    "\n",
    "Method 2: [4] Z. Yu and C. Zhang, “Image based static facial expression recognition with multiple deep network learning,” in ACM International Conference on Multimodal Interaction (MMI), 2015\n",
    "\n",
    "Method 3: [5] B.-K. Kim, S.-Y. Dong, J. Roh, G. Kim, and S.-Y. Lee, “Fusing Aligned and Non-Aligned Face Information for Automatic Affect Recognition in the Wild: A Deep Learning Approach\", 2016\n",
    "\n",
    "Method 4: [6] B.-K. Kim, J. Roh, S.-Y. Dong, and S.-Y. Lee, \"Hierarchical committee of deep convolutional neural networks for robust facial expression recognition\", 2016,\n",
    "\n",
    "Method 5: [7] A. Mollahosseini, D. Chan, and M. H. Mahoor, “Going Deeper in Facial Expression Recognition using Deep Neural Networks\", 2015\n",
    "\n",
    "Method 6: [8] Z. Zhang, P. Luo, C.-C. Loy, and X. Tang, “Learning Social Relation Traits from Face Images\", 2015\n",
    "\n",
    "ref*: [9] X. Xiong and F. Torre, “Supervised descent method and its applications to face alignment\", 2013\n",
    "\n",
    "[10] Christopher Pramerdorfer, Martin Kampel, \"Facial Expression Recognition using\n",
    "Convolutional Neural Networks: State of the Art\", https://pdfs.semanticscholar.org/4edc/7f27d4512b69be54abfc6b9876e5b00725ab.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
